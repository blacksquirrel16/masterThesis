{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ZkPJxk7xBf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY2mqdHA73Cb",
        "outputId": "e547f171-00de-4aed-803a-b4f43b699136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPZ8RVtg77mG"
      },
      "outputs": [],
      "source": [
        "file_path = 'renttherunway_final_data.json'\n",
        "df_rent = pd.read_json(file_path, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwSnNPHg8BUl"
      },
      "outputs": [],
      "source": [
        "df_rent_clean = df_rent.dropna(subset=[\"user_id\", \"item_id\", \"rating\",\"age\"]).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aszi5LOG8DDe"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# 3. Explicit Feedback\n",
        "###############################################\n",
        "\n",
        "df_rent_clean['interaction'] = df_rent_clean['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm5dJ79_xusC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "category_bow = vectorizer.fit_transform(df_rent_clean['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_iArRdP8FWj",
        "outputId": "05ed9950-6c68-4033-bb5f-e55132796d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'user_id': np.float64(0.0), 'item_id': np.float64(0.0), 'rating': np.float64(0.0), 'size': np.float64(0.0), 'age': np.float64(0.0), 'interaction': np.float64(0.0)}\n"
          ]
        }
      ],
      "source": [
        "def calculate_sparsity(df):\n",
        "    \"\"\"Calculates sparsity for each numerical feature in the DataFrame.\"\"\"\n",
        "    numerical_features = df.select_dtypes(include=np.number).columns\n",
        "    sparsity_results = {}\n",
        "\n",
        "    for feature in numerical_features:\n",
        "        sparsity = df[feature].isnull().sum() / len(df)\n",
        "        sparsity_results[feature] = sparsity\n",
        "\n",
        "    return sparsity_results\n",
        "\n",
        "\n",
        "sparsity_dict = calculate_sparsity(df_rent_clean)\n",
        "print(sparsity_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CCIOQsP8G6p"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, sparsity_threshold=0.50):\n",
        "    \"\"\"Preprocesses the data by removing numerical features with high sparsity.\"\"\"\n",
        "    for col in df.select_dtypes(include=np.number).columns:\n",
        "        sparsity = df[col].isnull().sum() / len(df)\n",
        "        if sparsity > sparsity_threshold:\n",
        "            print(f\"Dropping sparse feature: {col} (Sparsity: {sparsity:.2f})\")\n",
        "            df = df.drop(col, axis=1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoDvRaEk8I6H",
        "outputId": "345ae3eb-0fa1-4375-a1a5-6a8d034fb640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users with more than 2 interactions: 33544\n"
          ]
        }
      ],
      "source": [
        "df_rent_clean = preprocess_data(df_rent_clean, sparsity_threshold=0.50)\n",
        "\n",
        "user_interaction_counts = df_rent_clean.groupby('user_id')['rating'].count()\n",
        "users_with_enough_interactions = user_interaction_counts[user_interaction_counts >= 2].index\n",
        "df_rent_clean = df_rent_clean[df_rent_clean[\"user_id\"].isin(users_with_enough_interactions)]\n",
        "print(f\"Number of users with more than 2 interactions: {len(users_with_enough_interactions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY8WXyf8PLNn",
        "outputId": "cebfaafb-0b47-406d-e1e3-863c2f06b966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user_activity\n",
            "High    99471\n",
            "Low     20515\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "median_interaction_strength = df_rent_clean.groupby('user_id')['interaction'].sum().median()\n",
        "\n",
        "df_rent_clean['user_activity'] = df_rent_clean.groupby('user_id')['interaction'].transform('sum').map(\n",
        "    lambda x: 'Low' if x < median_interaction_strength else 'High'\n",
        ")\n",
        "\n",
        "print(df_rent_clean['user_activity'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zn9AzfA8OUe",
        "outputId": "ee3f7dc1-cbca-47d4-debb-3e541f516adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set shape: (71991, 17)\n",
            "Validation set shape: (23997, 17)\n",
            "Test set shape: (23998, 17)\n"
          ]
        }
      ],
      "source": [
        "df_rent_clean = df_rent_clean.sort_values('review_date')\n",
        "\n",
        "\n",
        "train_size = int(0.6 * len(df_rent_clean)) # 60 %\n",
        "val_size = int(0.2 * len(df_rent_clean))\n",
        "\n",
        "train_df = df_rent_clean[:train_size]\n",
        "val_df = df_rent_clean[train_size:train_size + val_size]\n",
        "test_df = df_rent_clean[train_size + val_size:]\n",
        "\n",
        "print(\"Train set shape:\", train_df.shape)\n",
        "print(\"Validation set shape:\", val_df.shape)\n",
        "print(\"Test set shape:\", test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQUKvBAs8XQ1"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# 3. User Item Matrix -Train\n",
        "###############################################\n",
        "\n",
        "train_user_item_matrix = train_df.pivot_table(\n",
        "    index=\"user_id\",\n",
        "    columns=\"item_id\",\n",
        "    values=\"interaction\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXTGcCdo8aDG"
      },
      "source": [
        "# Definition ndcg, hr, mrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvM85kiQ8ZpT"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# 3. Utility/Ranking metrics\n",
        "###############################################\n",
        "def dcg_at_k_recursive(relevance_scores, k, b=2):\n",
        "    \"\"\"\n",
        "    Calculate the Discounted Cumulative Gain at rank k recursively.\n",
        "\n",
        "    :param relevance_scores: A list or array of relevance scores for the ranked items.\n",
        "    :param k: The rank at which to stop (k items to evaluate).\n",
        "    :param b: The base of the logarithm (typically 2).\n",
        "    :return: DCG at rank k.\n",
        "    \"\"\"\n",
        "\n",
        "    k = min(k, len(relevance_scores))\n",
        "\n",
        "\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "\n",
        "    if k < b:\n",
        "        return np.sum([rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores[:k])])\n",
        "\n",
        "    else:\n",
        "        dcg_k_minus_1 = dcg_at_k_recursive(relevance_scores, k - 1, b)\n",
        "        rel_k = relevance_scores[k - 1]\n",
        "        return dcg_k_minus_1 + (rel_k / np.log2(k + 1))\n",
        "\n",
        "def dcg_at_k(relevance_scores, k, b=2):\n",
        "    \"\"\"\n",
        "    Calculate the Discounted Cumulative Gain at rank k (non-recursive).\n",
        "    \"\"\"\n",
        "    dcg = 0\n",
        "    for idx, rel in enumerate(relevance_scores[:k]):\n",
        "        dcg += rel / np.log2(idx + 2)\n",
        "    return dcg\n",
        "def idcg_at_k(relevance_scores, k):\n",
        "    \"\"\"\n",
        "    Calculate the Ideal Discounted Cumulative Gain at rank k.\n",
        "    :param relevance_scores: A list or array of relevance scores for the ranked items.\n",
        "    :param k: The rank at which to stop (k items to evaluate).\n",
        "    :return: Ideal DCG at rank k.\n",
        "    \"\"\"\n",
        "    relevance_scores = sorted(relevance_scores, reverse=True)\n",
        "    return dcg_at_k(relevance_scores, k)\n",
        "\n",
        "def ndcg_at_k(relevance_scores, k):\n",
        "    \"\"\"\n",
        "    Calculate the Normalized Discounted Cumulative Gain at rank k.\n",
        "    :param relevance_scores: A list or array of relevance scores for the ranked items.\n",
        "    :param k: The rank at which to stop (k items to evaluate).\n",
        "    :return: nDCG at rank k.\n",
        "    \"\"\"\n",
        "    dcg = dcg_at_k_recursive(relevance_scores, k)\n",
        "    idcg = idcg_at_k(relevance_scores, k)\n",
        "    if idcg == 0:\n",
        "        return 0\n",
        "    return dcg / idcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHGmF_gh8eKZ"
      },
      "outputs": [],
      "source": [
        "def hr_at_k(actual, predicted, k=5):\n",
        "    \"\"\"\n",
        "    Computes Hit Rate (HR) at rank k.\n",
        "\n",
        "    Args:\n",
        "        actual: list of relevant items\n",
        "        predicted: ranked list of items\n",
        "        k: rank cutoff (default is 5)\n",
        "\n",
        "    Returns:\n",
        "        Hit Rate at rank k. return q if relevant item is within topk\n",
        "    \"\"\"\n",
        "\n",
        "    for item in predicted[:k]:\n",
        "        if item in actual:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def hit_rate_at_k(actual, predicted, k=5):\n",
        "    \"\"\"\n",
        "    Computes Mean Hit Rate (HR) at rank k for all users.\n",
        "\n",
        "    Args:\n",
        "        actual: list of relevant items for each user\n",
        "        predicted: list of predicted ranked items for each user\n",
        "        k: rank cutoff (default is 5)\n",
        "\n",
        "    Returns:\n",
        "        HR at rank k.\n",
        "    \"\"\"\n",
        "\n",
        "    if not actual or len(predicted) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    hr_scores = []\n",
        "    for user_actual, user_predicted in zip(actual, predicted):\n",
        "        hr = hr_at_k(user_actual, user_predicted, k)\n",
        "        hr_scores.append(hr)\n",
        "\n",
        "    return np.mean(hr_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9aDAws-8gVg"
      },
      "outputs": [],
      "source": [
        "def rr_at_k(actual, predicted, k=5):\n",
        "    \"\"\"\n",
        "    Computes Reciprocal Rank at rank k.\n",
        "\n",
        "    Args:\n",
        "        actual: the relevant item(s)\n",
        "        predicted: the ranked list of items\n",
        "        k: rank cutoff (default is 5)\n",
        "\n",
        "    Returns:\n",
        "        Reciprocal Rank at rank k.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, item in enumerate(predicted[:k]):\n",
        "        if item == actual:\n",
        "            return 1 / (i + 1)\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def mrr_at_k(actual, predicted, k=5):\n",
        "    \"\"\"\n",
        "    Computes Mean Reciprocal Rank (MRR) at rank k.\n",
        "\n",
        "    Args:\n",
        "        actual: list of relevant items (can be multiple)\n",
        "        predicted: ranked list of items\n",
        "        k: rank cutoff (default is 5)\n",
        "\n",
        "    Returns:\n",
        "        MRR at rank k.\n",
        "    \"\"\"\n",
        "\n",
        "    if actual is None or len(predicted) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    rr_scores = []\n",
        "    for item in actual:\n",
        "        rr = rr_at_k(item, predicted, k)\n",
        "        rr_scores.append(rr)\n",
        "\n",
        "    return np.mean(rr_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnSb9Xsi8kup"
      },
      "source": [
        "# Fairness metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-dftlEG8h7t"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def calculate_disparate_impact(protected_outcomes, privileged_outcomes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        protected_outcomes: List of binary outcomes (1=favorable) for the protected group.\n",
        "        privileged_outcomes: List of binary outcomes for the privileged group.\n",
        "    Returns:\n",
        "        Disparate impact ratio.\n",
        "    \"\"\"\n",
        "    protected_rate = np.mean(protected_outcomes)\n",
        "    privileged_rate = np.mean(privileged_outcomes)\n",
        "\n",
        "    if privileged_rate == 0:\n",
        "        return np.inf\n",
        "\n",
        "    return protected_rate / privileged_rate\n",
        "\n",
        "\n",
        "def calculate_group_recommender_unfairness(group1_metrics, group2_metrics):\n",
        "  \"\"\"\n",
        "    Calculates the absolute difference in mean metrics between two groups.\n",
        "    This metric quantifies the unfairness of a recommender system by examining\n",
        "    the absolute difference in average performance between different user groups.\n",
        "\n",
        "    Args:\n",
        "        group1_metrics (list or numpy.ndarray): A list or numpy array of metrics for group 1.\n",
        "        group2_metrics (list or numpy.ndarray): A list or numpy array of metrics for group 2.\n",
        "\n",
        "    Returns:\n",
        "        float: The absolute difference in mean metrics between the two groups.\n",
        "  \"\"\"\n",
        "  return np.abs(np.mean(group1_metrics) - np.mean(group2_metrics))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OROQ4GNO8m6s"
      },
      "outputs": [],
      "source": [
        "def coefficient_of_variation(arr):\n",
        "    mean_val = np.mean(arr)\n",
        "    if mean_val == 0:\n",
        "        return 0\n",
        "    return np.std(arr) / mean_val\n",
        "\n",
        "def calculate_ucv(metric_low_group, metric_high_group):\n",
        "    # Calculate CV for each group then average\n",
        "    cv_low = coefficient_of_variation(metric_low_group) if len(metric_low_group) > 0 else 0\n",
        "    cv_high = coefficient_of_variation(metric_high_group) if len(metric_high_group) > 0 else 0\n",
        "    return (cv_low + cv_high) / 2\n",
        "\n",
        "def coefficient_of_variance(group):\n",
        "    \"\"\"Calculates the coefficient of variance for a group.\"\"\"\n",
        "    return np.std(group) / np.mean(group) if np.mean(group) != 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNUsKlvh8xfs"
      },
      "source": [
        "# Content based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gct5iWjQ8y5F",
        "outputId": "dd557c6b-1741-4c22-8b97-c44a63f418f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommended Items for User 991571 based on Category and Interaction Strength:\n",
            "        item_id category  rating   age\n",
            "125033   126335    dress     4.0  44.0\n",
            "140548   160612    dress    10.0  49.0\n",
            "33685    183200    dress    10.0  32.0\n",
            "6316     127495    dress     6.0  49.0\n",
            "89430    174086    dress    10.0  41.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "df_rent_clean['category'] = df_rent_clean['category'].fillna('unknown')\n",
        "\n",
        "\n",
        "def create_weighted_user_profile(user_id, df, interaction_data):\n",
        "    \"\"\"\n",
        "    Create a user profile based on the weighted interaction strengths for the categories\n",
        "    of the items the user has interacted with.\n",
        "    \"\"\"\n",
        "\n",
        "    user_items = interaction_data.loc[user_id, interaction_data.loc[user_id] > 0].index.tolist()\n",
        "\n",
        "\n",
        "    user_items_df = df[df['item_id'].isin(user_items)]\n",
        "\n",
        "    weighted_categories = user_items_df.groupby('category')['rating'].sum()\n",
        "\n",
        "    user_profile = ' '.join([f'{category} ' * int(weight) for category, weight in weighted_categories.items()])\n",
        "\n",
        "    return user_profile\n",
        "\n",
        "\n",
        "def recommend_items_for_user(user_id, df, interaction_data, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend items for a user based on their weighted profile similarity with all other items.\n",
        "    \"\"\"\n",
        "\n",
        "    user_profile = create_weighted_user_profile(user_id, df, interaction_data)\n",
        "    if not user_profile:\n",
        "        return ''\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "\n",
        "    all_profiles = df['category'].tolist() + [user_profile]\n",
        "\n",
        "    profile_matrix = vectorizer.fit_transform(all_profiles)\n",
        "\n",
        "    cosine_sim = cosine_similarity(profile_matrix[-1:], profile_matrix[:-1])\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_sim[0]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    item_indices = [i[0] for i in sim_scores[:top_n]]\n",
        "    item_scores = [i[1] for i in sim_scores[:top_n]]\n",
        "\n",
        "    recommended_items = df.iloc[item_indices]\n",
        "\n",
        "    return recommended_items[['item_id', 'category','rating','age']], item_scores\n",
        "\n",
        "\n",
        "recommended_items, scores = recommend_items_for_user(user_id=581157, df=df_rent_clean, interaction_data=train_user_item_matrix)\n",
        "\n",
        "print(\"Recommended Items for User 581157:\")\n",
        "print(recommended_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g2a9h6dPQ-G"
      },
      "outputs": [],
      "source": [
        "def update_user_profile(user_id, accepted_item, train_matrix, item_metadata=None):\n",
        "    \"\"\"\n",
        "    Updates a user's profile (train_matrix) by incrementing interaction with the accepted item.\n",
        "\n",
        "    Parameters:\n",
        "        user_id (int or str): ID of the user.\n",
        "        accepted_item (int or str): ID of the accepted item.\n",
        "        train_matrix (pd.DataFrame): User-item interaction matrix.\n",
        "        item_metadata (pd.DataFrame, optional): DataFrame with item metadata, including 'item_id' and 'category'.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if item_metadata is not None:\n",
        "        row = item_metadata[item_metadata['item_id'] == accepted_item]\n",
        "        if not row.empty:\n",
        "            item_category = row['category'].iloc[0]\n",
        "\n",
        "\n",
        "\n",
        "    if user_id not in train_matrix.index:\n",
        "        train_matrix.loc[user_id] = 0\n",
        "\n",
        "\n",
        "    if accepted_item not in train_matrix.columns:\n",
        "        train_matrix[accepted_item] = 0\n",
        "    train_matrix.loc[user_id, accepted_item] += 1\n",
        "\n",
        "    return train_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFLTwAUcOUyB"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics_for_user(user_id, actual_item, recommended_items, user_engaged, k=5):\n",
        "    hr = hr_at_k([actual_item], recommended_items, k)\n",
        "    mrr = mrr_at_k([actual_item], recommended_items, k)\n",
        "\n",
        "    relevance_scores = [1 if item == actual_item else 0 for item in recommended_items[:k]]\n",
        "    dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance_scores)])\n",
        "    idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(relevance_scores)))])\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0\n",
        "    cv = coefficient_of_variation(relevance_scores)\n",
        "\n",
        "    return {\n",
        "        'User': user_id,\n",
        "        'NDCG@K': ndcg,\n",
        "        'HR@K': hr,\n",
        "        'MRR@K': mrr,\n",
        "        'CV': cv,\n",
        "        'Engagement Group': user_engaged\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaLdzM-VjczO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "def coef_variation(arr):\n",
        "    mean_val = np.mean(arr)\n",
        "    return np.std(arr) / mean_val if mean_val != 0 else 0\n",
        "\n",
        "def create_weighted_user_profile(user_id, df, interaction_data):\n",
        "    if user_id not in interaction_data.index:\n",
        "        return ''\n",
        "    user_items = interaction_data.loc[user_id][interaction_data.loc[user_id] > 0].index.tolist()\n",
        "    user_items_df = df[df['item_id'].isin(user_items)].copy()\n",
        "    if user_items_df.empty:\n",
        "        return ''\n",
        "    most_recent_date = pd.to_datetime(user_items_df['review_date'].max())\n",
        "    user_items_df['review_date'] = pd.to_datetime(user_items_df['review_date'])\n",
        "    user_items_df['time_decay'] = np.exp(-(most_recent_date - user_items_df['review_date']).dt.days / 365)\n",
        "    weighted = user_items_df.groupby('category')[['interaction', 'time_decay']].apply(lambda g: (g['interaction'] * g['time_decay']).sum())\n",
        "    return ' '.join([f'{cat} ' * int(weight) for cat, weight in weighted.items()])\n",
        "\n",
        "def recommend_items_for_user_torch(user_id, df, interaction_data, top_n=5):\n",
        "    user_profile = create_weighted_user_profile(user_id, df, interaction_data)\n",
        "    if not user_profile.strip():\n",
        "        return []\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    all_profiles = df['category'].tolist() + [user_profile]\n",
        "    profile_matrix = vectorizer.fit_transform(all_profiles)\n",
        "    cosine_sim = cosine_similarity(profile_matrix[-1:], profile_matrix[:-1])\n",
        "    sim_scores = sorted(list(enumerate(cosine_sim[0])), key=lambda x: x[1], reverse=True)\n",
        "    item_indices = [i[0] for i in sim_scores[:top_n]]\n",
        "    return df.iloc[item_indices]['item_id'].tolist()\n",
        "\n",
        "def calculate_metrics_for_user(user_id, actual_item, recommended_items, user_engaged, k=5):\n",
        "    hr = hr_at_k([actual_item], recommended_items, k)\n",
        "    mrr = mrr_at_k([actual_item], recommended_items, k)\n",
        "    relevance = [1 if item == actual_item else 0 for item in recommended_items[:k]]\n",
        "    dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)])\n",
        "    idcg = sum([1 / np.log2(i + 2) for i in range(min(k, len(relevance)))])\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0\n",
        "    cv = coef_variation(relevance)\n",
        "    return {\n",
        "        'User': user_id,\n",
        "        'NDCG@K': ndcg,\n",
        "        'HR@K': hr,\n",
        "        'MRR@K': mrr,\n",
        "        'CV': cv,\n",
        "        'Engagement Group': user_engaged\n",
        "    }\n",
        "\n",
        "def update_user_profile(user_id, item_id, interaction_data, df):\n",
        "    if user_id in interaction_data.index and item_id in interaction_data.columns:\n",
        "        interaction_data.at[user_id, item_id] += 1\n",
        "    return interaction_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###############################################\n",
        "# HYPERPARAMETER\n",
        "###############################################\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf_max_df': [0.7, 0.8, 0.9, 1.0],\n",
        "    'tfidf_min_df': [0.0, 0.01, 0.05, 0.1],\n",
        "    'k': [5, 10, 15]\n",
        "}\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "\n",
        "best_params = None\n",
        "best_avg_ndcg = -1\n",
        "results = []\n",
        "\n",
        "def recommend_items_for_user_tuned(user_id, df, interaction_data, tfidf_vectorizer, top_n=5):\n",
        "    user_profile = create_weighted_user_profile(user_id, df, interaction_data)\n",
        "    all_categories = df['category'].tolist() + [user_profile]\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_categories)\n",
        "    user_vec = tfidf_matrix[-1]\n",
        "    item_vecs = tfidf_matrix[:-1]\n",
        "    cosine_sim = cosine_similarity(user_vec, item_vecs).flatten()\n",
        "    top_indices = cosine_sim.argsort()[::-1][:top_n]\n",
        "    recommended_items = df.iloc[top_indices]['item_id'].tolist()\n",
        "    return recommended_items\n",
        "\n",
        "\n",
        "for params in grid:\n",
        "    print(f\"Testing parameters: {params}\")\n",
        "\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                                       max_df=params['tfidf_max_df'],\n",
        "                                       min_df=params['tfidf_min_df'])\n",
        "\n",
        "    round_metrics = []\n",
        "    test_items = df_rent_clean.groupby(\"user_id\")[\"item_id\"].first().to_dict()\n",
        "\n",
        "    sample_users = list(test_items.keys())\n",
        "\n",
        "    for user_id in sample_users:\n",
        "        actual_item = test_items[user_id]\n",
        "        if user_id not in train_user_item_matrix.index:\n",
        "             continue\n",
        "\n",
        "        user_engagement = df_rent_clean[df_rent_clean[\"user_id\"] == user_id][\"user_activity\"].iloc[0]\n",
        "\n",
        "        recommended_items = recommend_items_for_user_tuned(user_id, df_rent_clean, train_user_item_matrix, tfidf_vectorizer, top_n=params['k'])\n",
        "        metrics = calculate_metrics_for_user(user_id, actual_item, recommended_items, user_engagement, k=params['k'])\n",
        "        round_metrics.append(metrics)\n",
        "\n",
        "    round_metrics_df = pd.DataFrame(round_metrics)\n",
        "    if not round_metrics_df.empty:\n",
        "        avg_ndcg = round_metrics_df['NDCG@K'].mean()\n",
        "        avg_hr = round_metrics_df['HR@K'].mean()\n",
        "        avg_mrr = round_metrics_df['MRR@K'].mean()\n",
        "    else:\n",
        "        avg_ndcg, avg_hr, avg_mrr = 0, 0, 0\n",
        "\n",
        "    print(f\"Average NDCG: {avg_ndcg}, Average HR: {avg_hr}, Average MRR: {avg_mrr}\")\n",
        "\n",
        "\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'avg_ndcg': avg_ndcg,\n",
        "        'avg_hr': avg_hr,\n",
        "        'avg_mrr': avg_mrr\n",
        "    })\n",
        "\n",
        "    if avg_ndcg > best_avg_ndcg:\n",
        "        best_avg_ndcg = avg_ndcg\n",
        "        best_params = params\n",
        "\n",
        "print(\"\\n--- Tuning Complete ---\")\n",
        "print(f\"Best parameters found: {best_params}\")\n",
        "print(f\"Best average NDCG: {best_avg_ndcg}\")\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResults for all parameter combinations:\")\n",
        "print(results_df)\n",
        "# Testing parameters: {'k': 5, 'tfidf_max_df': 1, 'tfidf_min_df': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NgvGDv2jgBd"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def run_rounds_with_warmup(df, train_matrix, rounds=3, warmup_fraction=0.1):\n",
        "    df['review_date'] = pd.to_datetime(df['review_date'])\n",
        "    df = df.sort_values(by='review_date')\n",
        "    train_matrix.index = train_matrix.index.astype(df['user_id'].dtype)\n",
        "\n",
        "    all_round_metrics = []\n",
        "    warmup_end = int(len(df) * warmup_fraction)\n",
        "    warmup_data = df.iloc[:warmup_end]\n",
        "    train_matrix_warmup = deepcopy(train_matrix)\n",
        "\n",
        "    for _, row in warmup_data.iterrows():\n",
        "        if row['user_id'] in train_matrix_warmup.index:\n",
        "            train_matrix_warmup = update_user_profile(row['user_id'], row['item_id'], train_matrix_warmup, df)\n",
        "\n",
        "    window_size = int((len(df) - warmup_end) / rounds)\n",
        "\n",
        "    for round_num in range(rounds):\n",
        "        print(f\"\\n--- Round {round_num + 1} ---\")\n",
        "        round_metrics = []\n",
        "\n",
        "        start = warmup_end + round_num * window_size\n",
        "        end = min(warmup_end + (round_num + 1) * window_size, len(df))\n",
        "        round_data = df.iloc[start:end]\n",
        "        historical_data = df.iloc[:start]\n",
        "\n",
        "        test_items = round_data.groupby(\"user_id\")[\"item_id\"].first().to_dict()\n",
        "\n",
        "        for user_id, actual_item in test_items.items():\n",
        "            if user_id not in train_matrix_warmup.index:\n",
        "                continue\n",
        "\n",
        "            engagement = round_data[round_data[\"user_id\"] == user_id][\"user_activity\"].iloc[0]\n",
        "            recommendations = recommend_items_for_user_torch(user_id, historical_data, train_matrix_warmup)\n",
        "            metrics = calculate_metrics_for_user(user_id, actual_item, recommendations, engagement)\n",
        "            round_metrics.append(metrics)\n",
        "\n",
        "            if round_num < rounds - 1:\n",
        "                train_matrix_warmup = update_user_profile(user_id, actual_item, train_matrix_warmup, df)\n",
        "\n",
        "        df_metrics = pd.DataFrame(round_metrics)\n",
        "\n",
        "        def get_ucv(metric_name):\n",
        "            low = df_metrics[df_metrics['Engagement Group'] == 'Low'][metric_name].values\n",
        "            high = df_metrics[df_metrics['Engagement Group'] != 'Low'][metric_name].values\n",
        "            return (coef_variation(low) + coef_variation(high)) / 2\n",
        "\n",
        "        ucv_ndcg = get_ucv('NDCG@K')\n",
        "        ucv_hr = get_ucv('HR@K')\n",
        "        ucv_mrr = get_ucv('MRR@K')\n",
        "\n",
        "        print(f\"UCV@K for NDCG: {ucv_ndcg:.6f}, HR: {ucv_hr:.6f}, MRR: {ucv_mrr:.6f}\")\n",
        "\n",
        "        low_hr_outcomes = df_metrics[df_metrics['Engagement Group'] == 'Low']['HR@K'].values\n",
        "        high_hr_outcomes = df_metrics[df_metrics['Engagement Group'] != 'Low']['HR@K'].values\n",
        "        di_hr = calculate_disparate_impact(low_hr_outcomes, high_hr_outcomes)\n",
        "        print(f\"Disparate Impact (HR@K): {di_hr:.6f}\")\n",
        "        low_ndcg_outcomes = df_metrics[df_metrics['Engagement Group'] == 'Low']['NDCG@K'].values\n",
        "        high_ndcg_outcomes = df_metrics[df_metrics['Engagement Group'] != 'Low']['NDCG@K'].values\n",
        "        di_ndcg = calculate_disparate_impact(low_ndcg_outcomes, high_ndcg_outcomes)\n",
        "        print(f\"Disparate Impact (NDCG@K): {di_ndcg:.6f}\")\n",
        "        gru_ndcg = calculate_group_recommender_unfairness(low_ndcg_outcomes, high_ndcg_outcomes)\n",
        "        print(f\"Group Recommender Unfairness (NDCG@K): {gru_ndcg:.6f}\")\n",
        "        low_mrr_outcomes = df_metrics[df_metrics['Engagement Group'] == 'Low']['MRR@K'].values\n",
        "        high_mrr_outcomes = df_metrics[df_metrics['Engagement Group'] != 'Low']['MRR@K'].values\n",
        "        di_mrr = calculate_disparate_impact(low_mrr_outcomes, high_mrr_outcomes)\n",
        "        print(f\"Disparate Impact (MRR@K): {di_mrr:.6f}\")\n",
        "        gru_mrr = calculate_group_recommender_unfairness(low_mrr_outcomes, high_mrr_outcomes)\n",
        "        print(f\"Group Recommender Unfairness (MRR@K): {gru_mrr:.6f}\")\n",
        "\n",
        "        gru_hr = calculate_group_recommender_unfairness(low_hr_outcomes, high_hr_outcomes)\n",
        "        print(f\"Group Recommender Unfairness (HR@K): {gru_hr:.6f}\")\n",
        "        avg_ndcg = df_metrics['NDCG@K'].mean()\n",
        "        avg_hr = df_metrics['HR@K'].mean()\n",
        "        avg_mrr = df_metrics['MRR@K'].mean()\n",
        "        print(f\"Average NDCG: {avg_ndcg:.6f}, Average HR: {avg_hr:.6f}, Average MRR: {avg_mrr:.6f}\")\n",
        "        grouped = df_metrics.groupby(\"Engagement Group\").agg({\n",
        "            \"NDCG@K\": \"mean\", \"HR@K\": \"mean\", \"MRR@K\": \"mean\", \"CV\": \"mean\"\n",
        "        })\n",
        "        grouped[\"UCV_NDCG\"] = ucv_ndcg\n",
        "        print(grouped)\n",
        "        all_round_metrics.append(grouped)\n",
        "\n",
        "    return all_round_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbkkdvPCjqJD",
        "outputId": "f2d9e4fe-b863-4818-b71d-5cc44b49556c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Round 1 ---\n",
            "UCV@K for NDCG: 6.551855, HR: 6.297242, MRR: 6.471418\n",
            "Disparate Impact (HR@K): 1.310962\n",
            "Disparate Impact (NDCG@K): 1.300394\n",
            "Group Recommender Unfairness (NDCG@K): 0.005288\n",
            "Disparate Impact (MRR@K): 1.258475\n",
            "Group Recommender Unfairness (MRR@K): 0.005314\n",
            "Group Recommender Unfairness (HR@K): 0.006710\n",
            "Average NDCG: 0.018932, Average HR: 0.023265, Average MRR: 0.021894\n",
            "                    NDCG@K      HR@K     MRR@K        CV  UCV_NDCG\n",
            "Engagement Group                                                  \n",
            "High              0.017603  0.021579  0.020558  0.020524  6.551855\n",
            "Low               0.022891  0.028289  0.025872  0.026010  6.551855\n",
            "\n",
            "--- Round 2 ---\n",
            "UCV@K for NDCG: 11.427074, HR: 10.572097, MRR: 11.314175\n",
            "Disparate Impact (HR@K): 1.582535\n",
            "Disparate Impact (NDCG@K): 1.518642\n",
            "Group Recommender Unfairness (NDCG@K): 0.002698\n",
            "Disparate Impact (MRR@K): 1.589911\n",
            "Group Recommender Unfairness (MRR@K): 0.003384\n",
            "Group Recommender Unfairness (HR@K): 0.004160\n",
            "Average NDCG: 0.005737, Average HR: 0.007967, Average MRR: 0.006407\n",
            "                    NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
            "Engagement Group                                                   \n",
            "High              0.005201  0.007141  0.005736  0.004878  11.427074\n",
            "Low               0.007899  0.011302  0.009119  0.009449  11.427074\n",
            "\n",
            "--- Round 3 ---\n",
            "UCV@K for NDCG: 17.236411, HR: 15.919690, MRR: 17.047350\n",
            "Disparate Impact (HR@K): 2.039093\n",
            "Disparate Impact (NDCG@K): 1.725419\n",
            "Group Recommender Unfairness (NDCG@K): 0.001507\n",
            "Disparate Impact (MRR@K): 1.798202\n",
            "Group Recommender Unfairness (MRR@K): 0.001892\n",
            "Group Recommender Unfairness (HR@K): 0.002951\n",
            "Average NDCG: 0.002374, Average HR: 0.003421, Average MRR: 0.002743\n",
            "                    NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
            "Engagement Group                                                   \n",
            "High              0.002077  0.002840  0.002371  0.001920  17.236411\n",
            "Low               0.003584  0.005792  0.004263  0.005124  17.236411\n",
            "[                    NDCG@K      HR@K     MRR@K        CV  UCV_NDCG\n",
            "Engagement Group                                                  \n",
            "High              0.017603  0.021579  0.020558  0.020524  6.551855\n",
            "Low               0.022891  0.028289  0.025872  0.026010  6.551855,                     NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
            "Engagement Group                                                   \n",
            "High              0.005201  0.007141  0.005736  0.004878  11.427074\n",
            "Low               0.007899  0.011302  0.009119  0.009449  11.427074,                     NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
            "Engagement Group                                                   \n",
            "High              0.002077  0.002840  0.002371  0.001920  17.236411\n",
            "Low               0.003584  0.005792  0.004263  0.005124  17.236411]\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "results = run_rounds_with_warmup(df_rent_clean, train_user_item_matrix, rounds=3, warmup_fraction=0.1)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddhNmkhC5Ed"
      },
      "source": [
        "R1:\n",
        "UCV@K for NDCG: 6.551855, HR: 6.297242, MRR: 6.471418\n",
        "Disparate Impact (HR@K): 1.310962\n",
        "Disparate Impact (NDCG@K): 1.300394\n",
        "Group Recommender Unfairness (NDCG@K): 0.005288\n",
        "Disparate Impact (MRR@K): 1.258475\n",
        "Group Recommender Unfairness (MRR@K): 0.005314\n",
        "Group Recommender Unfairness (HR@K): 0.006710\n",
        "                    NDCG@K      HR@K     MRR@K        CV  UCV_NDCG\n",
        "Engagement Group                                                  \n",
        "High              0.017603  0.021579  0.020558  0.020524  6.551855\n",
        "Low               0.022891  0.028289  0.025872  0.026010  6.551855\n",
        "\n",
        "\n",
        "R2:\n",
        "UCV@K for NDCG: 11.427074, HR: 10.572097, MRR: 11.314175\n",
        "Disparate Impact (HR@K): 1.582535\n",
        "Disparate Impact (NDCG@K): 1.518642\n",
        "Group Recommender Unfairness (NDCG@K): 0.002698\n",
        "Disparate Impact (MRR@K): 1.589911\n",
        "Group Recommender Unfairness (MRR@K): 0.003384\n",
        "Group Recommender Unfairness (HR@K): 0.004160\n",
        "                    NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
        "Engagement Group                                                   \n",
        "High              0.005201  0.007141  0.005736  0.004878  11.427074\n",
        "Low               0.007899  0.011302  0.009119  0.009449  11.427074\n",
        "\n",
        "R3: UCV@K for NDCG: 17.236411, HR: 15.919690, MRR: 17.047350\n",
        "Group Recommender Unfairness (NDCG@K): 0.001507\n",
        "Disparate Impact (MRR@K): 1.798202\n",
        "Group Recommender Unfairness (MRR@K): 0.001892\n",
        "Group Recommender Unfairness (HR@K): 0.002951\n",
        "                    NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
        "Engagement Group                                                   \n",
        "High              0.002077  0.002840  0.002371  0.001920  17.236411\n",
        "Low               0.003584  0.005792  0.004263  0.005124  17.236411\n",
        "[                    NDCG@K      HR@K     MRR@K        CV  UCV_NDCG\n",
        "Engagement Group                                                  \n",
        "High              0.017603  0.021579  0.020558  0.020524  6.551855\n",
        "Low               0.022891  0.028289  0.025872  0.026010  6.551855,                     NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
        "Engagement Group                                                   \n",
        "High              0.005201  0.007141  0.005736  0.004878  11.427074\n",
        "Low               0.007899  0.011302  0.009119  0.009449  11.427074,                     NDCG@K      HR@K     MRR@K        CV   UCV_NDCG\n",
        "Engagement Group                                                   \n",
        "High              0.002077  0.002840  0.002371  0.001920  17.236411\n",
        "Low               0.003584  0.005792  0.004263  0.005124  17.236411]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-6f0nqTz4Qe"
      },
      "source": [
        "Hybrid - in hybrid.py Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC9zRwDGz6wk"
      },
      "source": [
        "/usr/local/bin/python3.11 /Users/helengaiser/Documents/ucph/thesis/datasets/hybrid.py\n",
        "{'user_id': 0.0, 'item_id': 0.0, 'rating': 0.0, 'size': 0.0, 'age': 0.0, 'interaction': 0.0}\n",
        "Number of users with more than 2 interactions: 33544\n",
        "user_activity\n",
        "Low     2173\n",
        "High    1869\n",
        "Name: count, dtype: int64\n",
        "user_activity\n",
        "High    99471\n",
        "Low     20515\n",
        "Name: count, dtype: int64\n",
        "Train set shape: (71991, 17)\n",
        "Validation set shape: (23997, 17)\n",
        "Test set shape: (23998, 17)\n",
        "Recommended Items for User 991571 based on Category and Interaction Strength:\n",
        "        item_id category  rating   age\n",
        "125033   126335    dress     4.0  44.0\n",
        "140548   160612    dress    10.0  49.0\n",
        "33685    183200    dress    10.0  32.0\n",
        "6316     127495    dress     6.0  49.0\n",
        "89430    174086    dress    10.0  41.0\n",
        "Round 1\n",
        "Relevant Items: [126335, 160612]\n",
        "Excluded Items: {183200, 174086, 127495}\n",
        "Round 2\n",
        "Relevant Items: [126335, 160612]\n",
        "Excluded Items: {183200, 174086, 127495}\n",
        "Round 3\n",
        "Relevant Items: [126335, 160612]\n",
        "Excluded Items: {183200, 174086, 127495}\n",
        "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning:\n",
        "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
        "the same time. Both libraries are known to be incompatible and this\n",
        "can cause random crashes or deadlocks on Linux when loaded in the\n",
        "same Python program.\n",
        "Using threadpoolctl may cause crashes or deadlocks. For more\n",
        "information and possible workarounds, please see\n",
        "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
        "\n",
        "  warnings.warn(msg, RuntimeWarning)\n",
        "Distances shape: (27439, 5)\n",
        "Indices shape: (27439, 5)\n",
        "Hybrid Recommendations for User 623100:\n",
        "(144714, 0.443942765646799)\n",
        "(149739, 0.443942765646799)\n",
        "(145906, 0.443942765646799)\n",
        "(125564, 0.443942765646799)\n",
        "(2382109, 0.08872854958331756)\n",
        "Starting round 1 of hybrid recommendation...\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037880  0.063447  0.029545\n",
        "Low               0.052064  0.086877  0.040638\n",
        "Disparate Impact for NDCG: 1.374453403814944\n",
        "Disparate Impact for HR: 1.3692792455691942\n",
        "Disparate Impact for MRR: 1.3754923097864333\n",
        "GRU for NDCG: 0.014184214633436706\n",
        "GRU for HR: 0.023429719763331372\n",
        "GRU for MRR: 0.011093803575659662\n",
        "UCV for NDCG: 3.783842122989167\n",
        "UCV for HR: 3.542010191603885\n",
        "UCV for MRR: 4.193313252666606\n",
        "Average NDCG: 0.04183072954688187\n",
        "Average HR: 0.06997339553190714\n",
        "Average MRR: 0.03263481419390891\n",
        "Round 1 Metrics with UCV:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037880  0.063447  0.029545\n",
        "Low               0.052064  0.086877  0.040638\n",
        "\n",
        "\n",
        "Starting round 2 of hybrid recommendation...\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037863  0.063397  0.029539\n",
        "Low               0.052072  0.086877  0.040649\n",
        "Disparate Impact for NDCG: 1.3752626990625592\n",
        "Disparate Impact for HR: 1.3703703047290101\n",
        "Disparate Impact for MRR: 1.3761358571231965\n",
        "GRU for NDCG: 0.01420870503154023\n",
        "GRU for HR: 0.02348023501893856\n",
        "GRU for MRR: 0.011110600245139882\n",
        "UCV for NDCG: 3.784646535140646\n",
        "UCV for HR: 3.5428272097439013\n",
        "UCV for MRR: 4.193869171717489\n",
        "Average NDCG: 0.041821121318304115\n",
        "Average HR: 0.0699369510550676\n",
        "Average MRR: 0.03263359937801426\n",
        "Round 2 Metrics with UCV:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037863  0.063397  0.029539\n",
        "Low               0.052072  0.086877  0.040649\n",
        "\n",
        "\n",
        "Starting round 3 of hybrid recommendation...\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037924  0.063548  0.029572\n",
        "Low               0.052132  0.087008  0.040686\n",
        "Disparate Impact for NDCG: 1.3746302522383946\n",
        "Disparate Impact for HR: 1.3691612200820935\n",
        "Disparate Impact for MRR: 1.3758614607803659\n",
        "GRU for NDCG: 0.014207550854668523\n",
        "GRU for HR: 0.023459527928029578\n",
        "GRU for MRR: 0.011114836287170447\n",
        "UCV for NDCG: 3.7807913523847176\n",
        "UCV for HR: 3.539043707813361\n",
        "UCV for MRR: 4.190262158600773\n",
        "Average NDCG: 0.0418816395012446\n",
        "Average HR: 0.07008272896242575\n",
        "Average MRR: 0.032667614223064495\n",
        "Round 3 Metrics with UCV:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037924  0.063548  0.029572\n",
        "Low               0.052132  0.087008  0.040686\n",
        "\n",
        "\n",
        "\n",
        "Hybrid Metrics for Round 1:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037880  0.063447  0.029545\n",
        "Low               0.052064  0.086877  0.040638\n",
        "\n",
        "Hybrid Metrics for Round 2:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037863  0.063397  0.029539\n",
        "Low               0.052072  0.086877  0.040649\n",
        "\n",
        "Hybrid Metrics for Round 3:\n",
        "                    NDCG@K      HR@K     MRR@K\n",
        "Engagement Group                              \n",
        "High              0.037924  0.063548  0.029572\n",
        "Low               0.052132  0.087008  0.040686\n",
        "\n",
        "--- Fairness Metrics for Round 1 ---\n",
        "NDCG:\n",
        "  Disparate Impact (Low/High): 1.3745\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0142\n",
        "MRR:\n",
        "  Disparate Impact (Low/High): 1.3755\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0111\n",
        "HR@K:\n",
        "  Disparate Impact (Low/High): 1.3693\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0234\n",
        "\n",
        "--- Fairness Metrics for Round 2 ---\n",
        "NDCG:\n",
        "  Disparate Impact (Low/High): 1.3753\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0142\n",
        "MRR:\n",
        "  Disparate Impact (Low/High): 1.3761\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0111\n",
        "HR@K:\n",
        "  Disparate Impact (Low/High): 1.3704\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0235\n",
        "\n",
        "--- Fairness Metrics for Round 3 ---\n",
        "NDCG:\n",
        "  Disparate Impact (Low/High): 1.3746\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0142\n",
        "MRR:\n",
        "  Disparate Impact (Low/High): 1.3759\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0111\n",
        "HR@K:\n",
        "  Disparate Impact (Low/High): 1.3692\n",
        "  Group Recommender Unfairness (Abs Diff): 0.0235\n",
        "\n",
        "Process finished with exit code 0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
